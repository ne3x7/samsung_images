{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, binary classification on images will be performed. First, a baseline based on PCA algorithm and Logistic Regression will be introduced. Then, a Convolutional Neural Network will be trained to improve classification quality.\n",
    "\n",
    "I used several sources of knowledge. For the PCA part, MIPT&Coursera [\"Unsupervised learning\" course](https://www.coursera.org/learn/unsupervised-learning/home/welcome), and for the CNN part, Google&Udacity [\"Deep Learning\" course](https://www.udacity.com/course/deep-learning--ud730) and general architecture like proposed in [ImageNet Classification With Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "from scipy import ndimage\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.despine()\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is RGB images of size $32 \\times 32$. The `ndimage` module does all the work for reading images into 3-dimensional arrays, which are then organized into one 4-dimensional collection. Finally, train/validation/test split is performed.\n",
    "\n",
    "I keep images RGB to make use of as much information as possible. Since the data size is small, it doesn't affect performance very much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "pixel_depth = 255.\n",
    "num_channels = 3\n",
    "\n",
    "def load_class(name):\n",
    "    \"\"\"\n",
    "    Loads one class into memory.\n",
    "    \n",
    "    Args:\n",
    "        name (string): Class name.\n",
    "\n",
    "    Returns:\n",
    "        images: Tensor of shape (num_images, image_size, image_size).\n",
    "    \"\"\"\n",
    "    assert name in ['clocks', 'crocodiles'], 'Class name %s not recognized' % name\n",
    "    image_files = os.listdir('./data/%s/' % name)\n",
    "    images = np.ndarray(shape=(len(image_files), image_size, image_size, num_channels), dtype=np.float32)\n",
    "    num_images = 0\n",
    "        \n",
    "    for image in image_files:\n",
    "        path = os.path.join('./data/%s/' % name, image)\n",
    "        try:\n",
    "            image_data = (ndimage.imread(path).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (image_size, image_size, num_channels):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            images[num_images, :, :, :] = image_data\n",
    "            num_images += 1\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file, ':', e, 'skipping.')\n",
    "    \n",
    "    images = images[0:num_images, :, :, :]\n",
    "    print('Class %s tensor:' % name, images.shape)\n",
    "    return images\n",
    "\n",
    "def load_classes(force=False):\n",
    "    \"\"\"\n",
    "    Pickles classes.\n",
    "    \n",
    "    Args:\n",
    "        force (bool): True if rewrite existing pickles, False otherwise.\n",
    "        \n",
    "    Returns:\n",
    "        names (list of strings): Class names.\n",
    "    \"\"\"\n",
    "    names = ['clocks', 'crocodiles']\n",
    "    for name in names:\n",
    "        if os.path.exists('./data/%s.pickle' % name) and not force:\n",
    "            print('%s.pickle already present, skipping.' % name)\n",
    "        else:\n",
    "            print 'Pickling %s.pickle' % name\n",
    "            images = load_class(name)\n",
    "            try:\n",
    "                with open('./data/%s.pickle' % name, 'w') as fout:\n",
    "                    pickle.dump(images, fout, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', '%s.pickle' % name, ':', e)\n",
    "    return names\n",
    "\n",
    "def make_arrays(rows, cols, depth):\n",
    "    \"\"\"\n",
    "    Allocates memory for a tensor of specified length.\n",
    "    \n",
    "    Args:\n",
    "        rows (int): First dimension.\n",
    "        cols (int): Second and third dimension.\n",
    "        depth (int): Fourth dimension.\n",
    "\n",
    "    Returns:\n",
    "        dataset (list): Tensor of shape (rows, image_size, image_size).\n",
    "        labels (list): Tensor of shape (rows).\n",
    "    \"\"\"\n",
    "    if rows:\n",
    "        dataset = np.ndarray((rows, cols, cols, depth), dtype=np.float32)\n",
    "        labels = np.ndarray(rows, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels\n",
    "\n",
    "def merge_classes(names, train_size, valid_size=0, test_size=0):\n",
    "    \"\"\"\n",
    "    Merges classes into one dataset, performs train/validation split.\n",
    "    \n",
    "    Args:\n",
    "        names (list of strings): Class names.\n",
    "        train_size (int): Amount of data to keep for training.\n",
    "        valid_size (int): Amount of data to keep for validation.\n",
    "        valid_size (int): Amount of data to keep for test.\n",
    "\n",
    "    Returns:\n",
    "        valid_dataset (list): Tensor of shape (valid_size, image_size, image_size, num_channels).\n",
    "        valid_labels (list): Tensor of shape (valid_size).\n",
    "        train_dataset (list): Tensor of shape (train_size, image_size, image_size, num_channels).\n",
    "        train_labels (list): Tensor of shape (train_size).\n",
    "        test_dataset (list): Tensor of shape (test_size, image_size, image_size, num_channels).\n",
    "        test_labels (list): Tensor of shape (test_size).\n",
    "    \"\"\"\n",
    "    num_classes = len(names)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_size, num_channels)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_size, num_channels)\n",
    "    test_dataset, test_labels = make_arrays(test_size, image_size, num_channels)\n",
    "    vsize_per_class = valid_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "    ttsize_per_class = test_size // num_classes\n",
    "    \n",
    "    start_v, start_t, start_tt = 0, 0, 0\n",
    "    end_v, end_t, end_tt = vsize_per_class, tsize_per_class, ttsize_per_class\n",
    "    end_l = vsize_per_class + tsize_per_class + ttsize_per_class\n",
    "    for label, src in enumerate(names):\n",
    "        try:\n",
    "            with open('./data/%s.pickle' % src, 'r') as fin:\n",
    "                images = pickle.load(fin)\n",
    "                np.random.shuffle(images)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_images = images[:vsize_per_class, :, :, :]\n",
    "                    valid_dataset[start_v:end_v, :, :, :] = valid_images\n",
    "                    valid_labels[start_v:end_v] = label\n",
    "                    start_v += vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "                    \n",
    "                if test_dataset is not None:\n",
    "                    test_images = images[vsize_per_class:vsize_per_class+ttsize_per_class, :, :, :]\n",
    "                    test_dataset[start_tt:end_tt, :, :, :] = test_images\n",
    "                    test_labels[start_tt:end_tt] = label\n",
    "                    start_tt += ttsize_per_class\n",
    "                    end_tt += ttsize_per_class\n",
    "                    \n",
    "                train_images = images[vsize_per_class+ttsize_per_class:end_l, :, :, :]\n",
    "                train_dataset[start_t:end_t, :, :, :] = train_images\n",
    "                train_labels[start_t:end_t] = label\n",
    "                start_t += tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', src, ':', e)\n",
    "            raise\n",
    "    \n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels, test_dataset, test_labels\n",
    "\n",
    "def shuffle(dataset, labels):\n",
    "    \"\"\"\n",
    "    Shuffles dataset w.r.t. labels.\n",
    "    \n",
    "    Args:\n",
    "        dataset (list): 3D-Tensor to shuffle.\n",
    "        labels (list): 1D-Tensor to shuffle.\n",
    "\n",
    "    Returns:\n",
    "        shuffled_dataset (list): Shuffled Tensor `dataset`.\n",
    "        shuffled_labels (list): Shuffled Tensor `labels`.\n",
    "    \"\"\"\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "def load_data(filename, save=None):\n",
    "    \"\"\"\n",
    "    Saves/loads all necessary data for clocks&crocs task.\n",
    "    \n",
    "    Args:\n",
    "        filename (string, optional): Filename to save/load.\n",
    "        save (dict, optional): Dictionary to save.\n",
    "        \n",
    "    Returns:\n",
    "        save (dict): Data dictionaty.\n",
    "    \"\"\"\n",
    "    pickle_file = './data/%s' % filename\n",
    "    if not os.path.exists(pickle_file):\n",
    "        try:\n",
    "            with open(pickle_file, 'w') as fout:\n",
    "                pickle.dump(save, fout, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', pickle_file, ':', e)\n",
    "            raise\n",
    "    else:\n",
    "        with open(pickle_file, 'r') as fin:\n",
    "            save = pickle.load(fin)\n",
    "    return save\n",
    "            \n",
    "def one_hot(labels, depth=2):\n",
    "    \"\"\"\n",
    "    One-hot `labels` into vector of given `depth`.\n",
    "    \n",
    "    Args:\n",
    "        labels (list): Values to one-hot.\n",
    "        depth (int): Depth of resulting vector.\n",
    "        \n",
    "    Return:\n",
    "        One-hot representation of `labels`.\n",
    "    \"\"\"\n",
    "    labels = (np.arange(depth) == labels[:,None]).astype(np.float32)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clocks.pickle already present, skipping.\n",
      "crocodiles.pickle already present, skipping.\n"
     ]
    }
   ],
   "source": [
    "total_size = 1000\n",
    "test_size = int(0.1 * total_size)\n",
    "valid_size = int(0.1 * total_size)\n",
    "train_size = total_size - valid_size - test_size\n",
    "\n",
    "names = load_classes()\n",
    "valid_dataset, valid_labels, train_dataset, train_labels, test_dataset, test_labels = merge_classes(names,\n",
    "                                                                                                    train_size,\n",
    "                                                                                                    valid_size,\n",
    "                                                                                                    test_size)\n",
    "\n",
    "# valid_dataset, valid_labels = shuffle(valid_dataset, valid_labels)\n",
    "# train_dataset, train_labels = shuffle(train_dataset, train_labels)\n",
    "# test_dataset, test_labels = shuffle(test_dataset, test_labels)\n",
    "\n",
    "# save = {'train_dataset': train_dataset, 'train_labels': train_labels, \n",
    "#         'valid_dataset': valid_dataset, 'valid_labels': valid_labels, \n",
    "#         'test_dataset': test_dataset, 'test_labels': test_labels}\n",
    "# save = load_data('data.pickle', save=save)\n",
    "\n",
    "save = load_data('data.pickle')\n",
    "train_dataset = save['train_dataset']\n",
    "train_labels = save['train_labels']\n",
    "valid_dataset = save['valid_dataset']\n",
    "valid_labels = save['valid_labels']\n",
    "test_dataset = save['test_dataset']\n",
    "test_labels = save['test_labels']\n",
    "del save\n",
    "\n",
    "train_labels = one_hot(train_labels)\n",
    "valid_labels = one_hot(valid_labels)\n",
    "test_labels = one_hot(test_labels)\n",
    "\n",
    "train_pca_feed = np.concatenate([train_dataset, valid_dataset]).reshape((train_dataset.shape[0]\n",
    "                                                                         + valid_dataset.shape[0], -1))\n",
    "train_pca_labels = [0 if l[0] == 1 else 1 for l in np.concatenate([train_labels, valid_labels])]\n",
    "test_pca_feed = test_dataset.reshape((test_dataset.shape[0], -1))\n",
    "test_pca_labels = [0 if l[0] == 1 else 1 for l in test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis is a powerful technique for dimension reduction. It projects high-dimensional data into smaller-dimensional space losing the least possible variance in data. PCA objective can be most easily obtained with SVD decomposition, then eigenvalue descending sorting and finally cutting projection matrices by specified number of components. Here, though, I use `sklearn` implementation, as it is a baseline.\n",
    "\n",
    "To calculate the optimal number of components, `sklearn` default scoring (log likelyhood) is used. To calculate classification quality accuracy metric is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = np.arange(1, train_pca_feed.shape[1], 5, dtype=np.int32)\n",
    "\n",
    "if os.path.exists('./data/pca_scores.pickle'):\n",
    "    with open('./data/pca_scores.pickle', 'r') as fin:\n",
    "        scores = pickle.load(fin)\n",
    "else:\n",
    "    scores = []\n",
    "    \n",
    "    for d in ds:\n",
    "        pca = PCA(n_components=d)\n",
    "        scores.append(cross_val_score(pca, train_pca_feed, cv=3, n_jobs=4).mean())\n",
    "    \n",
    "    with open('./data/pca_scores.pickle', 'w') as fout:\n",
    "        pickle.dump(fout, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('PCA scores (higher is better)', fontsize='xx-large')\n",
    "plt.plot(scores, label='PCA scores', fontsize='xx-large')\n",
    "plt.xlim(n_components[0], n_components[-1])\n",
    "plt.xlabel('Number of components', fontsize='xx-large')\n",
    "plt.ylabel('Log likelyhood', fontsize='xx-large')\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_components = 50\n",
    "pca = PCA(n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = pca.fit(train_pca_feed).transform(train_pca_feed)\n",
    "X_test = pca.transform(test_pca_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "preds = lr.predict(X_test)\n",
    "print 'Classifier accuracy %.2f' % accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks are arguably the most advanced and powerful way to deal with images. The main idea is to share the weights of a small fully-connected network on the whole image. This can be accomplished using convolutions or stacks of concolutions to make the network deeper.\n",
    "\n",
    "Here I use Google's `tensorflow` implementation of convolutions and an architecture of three convolutional layers followed by max-pooling layers and one fully-connected layer. Quality is measured using accuracy metric on validation set during training and on test set after convergence.\n",
    "\n",
    "During training, dropout is used to force the network rely strongly on data. The optimizer was initially `AdamOptimizer`, but was replaced with traditional `GradientDescentOptimizer` to improve quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(X, W, b, stride=1):\n",
    "    \"\"\"\n",
    "    Wrapper for convolution layer: convolution, bias adding and relu activation.\n",
    "    \n",
    "    Args:\n",
    "        X: Tensor to apply convolutional layer to.\n",
    "        W: Filter matrix.\n",
    "        b: Bias vector.\n",
    "        stride (optional): Filter stride.\n",
    "        \n",
    "    Returns:\n",
    "        Y: Result of convolution.\n",
    "    \"\"\"\n",
    "    X = tf.nn.conv2d(X, W, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    X = tf.nn.bias_add(X, b)\n",
    "    return tf.nn.relu(X)\n",
    "\n",
    "def maxpool(X, k=2):\n",
    "    \"\"\"\n",
    "    Wrapper for max pooling layer.\n",
    "    \n",
    "    Args:\n",
    "        X: Tensor to apply max pooling layer to.\n",
    "        k (optional): Size and stride of pooling.\n",
    "        \n",
    "    Returns:\n",
    "        Y: Result of max pooling.\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(X, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "def accuracy(true, predicted):\n",
    "    \"\"\"\n",
    "    Outputs accuracy rate for one-hot vectors.\n",
    "    \n",
    "    Args:\n",
    "        true (list): True labels.\n",
    "        predcited (list): Predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "        accuracy (float): Resulting value in [0, 1].\n",
    "    \"\"\"\n",
    "    return (np.sum(np.argmax(predicted, 1) == np.argmax(true, 1)) / float(predicted.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth1 = 32\n",
    "depth2 = 64\n",
    "depth3 = 1024\n",
    "num_labels = 2\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables\n",
    "    wc1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth1], stddev=0.1), name='wc1')\n",
    "    bc1 = tf.Variable(tf.zeros([depth1]), name='bc1')\n",
    "    wc2 = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth1, depth2], stddev=0.1), name='wc2')\n",
    "    bc2 = tf.Variable(tf.constant(1.0, shape=[depth2]), name='bc2')\n",
    "    wd = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth2, depth3], stddev=0.1), name='wd')\n",
    "    bd = tf.Variable(tf.constant(1.0, shape=[depth3]), name='bd')\n",
    "    wout = tf.Variable(tf.truncated_normal([depth3, num_labels], stddev=0.1), name='wout')\n",
    "    bout = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='bout')\n",
    "  \n",
    "    # Model\n",
    "    def model(data, training=False):\n",
    "        conv1 = conv2d(data, wc1, bc1)\n",
    "        conv1 = maxpool(conv1)\n",
    "        \n",
    "        conv2 = conv2d(conv1, wc2, bc2)\n",
    "        conv2 = maxpool(conv2)\n",
    "        \n",
    "        shape = conv2.get_shape().as_list()\n",
    "        dense = tf.reshape(conv2, [-1, shape[1] * shape[2] * shape[3]])\n",
    "        dense = tf.nn.bias_add(tf.matmul(dense, wd), bd)\n",
    "        dense = tf.nn.relu(dense)\n",
    "        \n",
    "        if training:\n",
    "            dense = tf.nn.dropout(dense, 0.5)\n",
    "        \n",
    "        return tf.nn.bias_add(tf.matmul(dense, wout), bout)\n",
    "  \n",
    "    # Training computation\n",
    "    logits = model(tf_train_dataset, training=True)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-2).minimize(loss)\n",
    "    tf.add_to_collection('ops', optimizer)\n",
    "  \n",
    "    # Predictions for the traininga and validation data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "    \n",
    "    # Saving\n",
    "    to_save = {'wc1': wc1, 'bc1': bc1, 'wc2': wc2, 'bc2': bc2, 'wd': wd, 'bd': bd, 'wout': wout, 'bout': bout}\n",
    "    \n",
    "    saver = tf.train.Saver(to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps = 100001\n",
    "losses = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        average_loss = 0\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 100 == 0:\n",
    "            losses.append(l)\n",
    "        if step % 500 == 0:\n",
    "            print 'Average loss step %d: %.3f' % (step, average_loss)\n",
    "            average_loss = 0\n",
    "            print 'Minibatch accuracy: %.3f' % accuracy(batch_labels, predictions)\n",
    "            print 'Validation accuracy: %.3f' % accuracy(valid_labels, valid_prediction.eval())\n",
    "        if len(losses) > 2 and abs(losses[-2] - losses[-1]) < 1e-5:\n",
    "            print 'Converged'\n",
    "            break\n",
    "    test_preds = test_predictions.eval()\n",
    "    print 'Test accuracy: %.3f' % accuracy(test_labels, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.semilogy(np.linspace(0, len(losses) * 100, len(losses)), losses, label='Loss')\n",
    "sns.\n",
    "plt.xlabel('Epoch', fontsize='xx-large')\n",
    "plt.ylabel('Loss (cross-entropy)', fontsize='xx-large')\n",
    "plt.legend(fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "view_idx = np.random.permutation(range(len(test_dataset)))[:25]\n",
    "view_dataset = test_dataset[view_idx, :, :, :]\n",
    "view_labels = y_test[view_idx]\n",
    "for i in range(len(view_dataset)):\n",
    "    plt.subplot(5, 5, 5 * i + i)\n",
    "    plt.title('True: %s, predicted: %s' % (names[test_preds[view_idx[i]]], names[view_labels[i]]))\n",
    "    plt.imshow(view_dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
